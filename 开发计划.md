# GEO监控系统开发计划

## 项目概述

基于现有技术白皮书和项目架构，本开发计划旨在构建一个完整的品牌声量与准确性实时监控平台，支持对ChatGPT、Claude、Gemini等主流AI模型的监控分析。

### 技术栈总览

**后端技术栈:**
- FastAPI (Python) - API网关和核心服务
- PostgreSQL - 主数据库 (生产环境使用Supabase)
- Redis - 消息队列和缓存 (生产环境使用Upstash)
- SQLAlchemy - ORM框架
- APScheduler - 任务调度
- OpenRouter - 多模型API网关

**前端技术栈:**
- Next.js 14 - React框架
- TypeScript - 类型安全
- TailwindCSS - 样式框架
- Tanstack Query - 数据获取和缓存
- Zustand - 状态管理
- Recharts - 数据可视化
- React Hook Form + Zod - 表单处理和验证
- Lucide React - 图标库

**基础设施:**
- Docker Compose - 本地开发环境
- Supabase - 生产数据库和认证
- Railway/Vercel - 部署平台
- Grafana - 监控和可观测性

## 开发阶段规划

### Phase 1: 核心功能完善 (4-6周)

#### 1.1 后端API完善 (2周)
**目标:** 完善核心API接口，确保所有基础功能可用

**任务清单:**
- [ ] **认证系统优化**
  - 完善JWT认证中间件
  - 实现多租户隔离机制
  - 添加用户权限管理
  - 集成Supabase Auth

- [ ] **监控任务管理API**
  - 任务CRUD操作完善
  - 关键词和模型关联管理
  - Cron调度表达式验证
  - 任务状态管理和监控

- [ ] **模型执行引擎**
  - OpenRouter API集成优化
  - 模型优先级和降级机制
  - 速率限制和重试逻辑
  - 成本控制和熔断机制

- [ ] **指标计算服务**
  - SOV计算算法实现
  - 准确性评分系统
  - 情感分析集成
  - 引用率统计

**验收标准:**
- 所有API端点响应时间 < 500ms
- 支持并发100个请求
- 错误率 < 1%
- API文档完整且可测试

#### 1.2 数据库架构优化 (1周)
**目标:** 优化数据库设计，提升查询性能

**任务清单:**
- [ ] **Schema优化**
  - 实施v1.1版本的关系型重构
  - 创建必要的索引
  - 设置RLS策略
  - 配置级联删除

- [ ] **数据迁移脚本**
  - 编写数据库初始化脚本
  - 创建测试数据生成器
  - 实现数据备份和恢复机制

**验收标准:**
- 查询响应时间 < 100ms
- 支持多租户数据隔离
- 数据一致性检查通过

#### 1.3 前端基础框架 (2-3周)
**目标:** 构建美观、现代的用户界面

**任务清单:**
- [ ] **UI组件库建设**
  - 基于shadcn/ui构建组件库
  - 实现响应式布局系统
  - 创建主题和样式系统
  - 建立设计规范文档

- [ ] **核心页面开发**
  - 仪表盘页面 (Dashboard)
  - 任务配置页面 (Task Management)
  - 数据分析页面 (Analytics)
  - 设置页面 (Settings)

- [ ] **数据可视化**
  - SOV趋势图表
  - 模型对比分析图
  - 实时监控面板
  - 告警状态展示

**验收标准:**
- 页面加载时间 < 2秒
- 移动端适配完成
- 用户体验流畅
- 设计美观现代

### Phase 2: 高级功能开发 (3-4周)

#### 2.1 实时监控和告警系统 (2周)
**任务清单:**
- [ ] **实时数据推送**
  - WebSocket连接管理
  - 实时指标更新
  - 状态变化通知

- [ ] **告警系统**
  - 阈值配置管理
  - 多渠道通知 (Email, Webhook, 站内信)
  - 告警规则引擎
  - 告警历史记录

- [ ] **任务调度优化**
  - 分布式任务调度
  - 任务失败重试机制
  - 任务执行监控

#### 2.2 高级分析功能 (2周)
**任务清单:**
- [ ] **深度分析报表**
  - 竞品对比分析
  - 趋势预测算法
  - 定位词命中率分析
  - 自定义报表生成

- [ ] **数据导出功能**
  - PDF报表生成
  - CSV数据导出
  - API数据接口
  - 定时报表推送

### Phase 3: 性能优化和部署 (2-3周)

#### 3.1 性能优化 (1-2周)
**任务清单:**
- [ ] **后端性能优化**
  - 数据库查询优化
  - 缓存策略实施
  - API响应时间优化
  - 内存使用优化

- [ ] **前端性能优化**
  - 代码分割和懒加载
  - 图片和资源优化
  - 缓存策略优化
  - 首屏加载优化

#### 3.2 生产环境部署 (1周)
**任务清单:**
- [ ] **云服务配置**
  - Supabase项目配置
  - Upstash Redis配置
  - OpenRouter API配置
  - 环境变量管理

- [ ] **CI/CD流水线**
  - GitHub Actions配置
  - 自动化测试
  - 自动化部署
  - 监控和告警

## 详细功能规格

### 1. 用户界面设计规范

#### 1.1 设计原则
- **简洁现代:** 采用现代扁平化设计风格
- **数据驱动:** 突出数据可视化和关键指标
- **响应式:** 支持桌面端、平板和移动端
- **可访问性:** 遵循WCAG 2.1标准

#### 1.2 色彩系统
```css
/* 主色调 */
--primary: 220 14% 96%;      /* 主品牌色 */
--primary-foreground: 220 9% 46%;

/* 功能色 */
--success: 142 76% 36%;      /* 成功状态 */
--warning: 38 92% 50%;       /* 警告状态 */
--error: 0 84% 60%;          /* 错误状态 */
--info: 221 83% 53%;         /* 信息状态 */

/* 中性色 */
--background: 0 0% 100%;     /* 背景色 */
--foreground: 222.2 84% 4.9%; /* 前景色 */
--muted: 210 40% 98%;        /* 静音色 */
--border: 214.3 31.8% 91.4%; /* 边框色 */
```

#### 1.3 组件规范
- **按钮:** 统一的按钮样式，支持多种尺寸和状态
- **表单:** 一致的表单控件样式和验证反馈
- **卡片:** 统一的卡片布局和阴影效果
- **图表:** 一致的图表配色和交互方式

### 2. 核心功能模块

#### 2.1 仪表盘 (Dashboard)
**功能描述:** 提供系统整体概览和关键指标展示

**核心组件:**
- **概览卡片组:** 显示总任务数、活跃监控、告警数量等
- **SOV趋势图:** 展示品牌声量份额的时间趋势
- **模型对比图:** 不同AI模型的表现对比
- **最新告警列表:** 显示最近的告警信息
- **成本监控:** 显示API调用成本和预算使用情况

**技术实现:**
```typescript
// 仪表盘数据结构
interface DashboardData {
  overview: {
    totalTasks: number;
    activeTasks: number;
    totalAlerts: number;
    monthlySpend: number;
  };
  sovTrend: Array<{
    date: string;
    sov: number;
    model: string;
  }>;
  modelComparison: Array<{
    model: string;
    accuracy: number;
    sov: number;
    cost: number;
  }>;
  recentAlerts: Array<{
    id: string;
    type: 'accuracy' | 'sov' | 'cost';
    message: string;
    timestamp: string;
    severity: 'low' | 'medium' | 'high';
  }>;
}
```

#### 2.2 任务管理 (Task Management)
**功能描述:** 创建、编辑和管理监控任务

**核心功能:**
- **任务创建向导:** 分步骤引导用户创建监控任务
- **关键词管理:** 支持单个添加和批量导入关键词
- **模型选择:** 支持多模型选择和优先级设置
- **调度配置:** 可视化Cron表达式编辑器
- **任务状态监控:** 实时显示任务执行状态

**表单验证规则:**
```typescript
const taskSchema = z.object({
  name: z.string().min(1, "任务名称不能为空").max(100, "任务名称过长"),
  keywords: z.array(z.string()).min(1, "至少需要一个关键词"),
  models: z.array(z.string()).min(1, "至少选择一个模型"),
  schedule: z.string().regex(/^[0-9\s\*\/\-\,]+$/, "无效的Cron表达式"),
  isActive: z.boolean(),
  alertThresholds: z.object({
    sovMin: z.number().min(0).max(100),
    accuracyMin: z.number().min(1).max(10),
  }),
});
```

#### 2.3 数据分析 (Analytics)
**功能描述:** 深度分析品牌表现和竞品对比

**核心功能:**
- **时间范围选择:** 支持自定义时间范围分析
- **多维度筛选:** 按关键词、模型、品牌等维度筛选
- **竞品对比:** 横向对比不同品牌的表现
- **趋势分析:** 识别数据趋势和异常点
- **导出功能:** 支持PDF报表和CSV数据导出

#### 2.4 设置管理 (Settings)
**功能描述:** 系统配置和用户偏好设置

**核心功能:**
- **API密钥管理:** OpenRouter等第三方服务密钥配置
- **通知设置:** 告警通知渠道和频率配置
- **用户权限:** 团队成员权限管理
- **系统配置:** 全局参数和限制设置

### 3. 后端API设计

#### 3.1 认证和授权
```python
# JWT Token结构
{
  "sub": "user_id",
  "tenant_id": "tenant_uuid",
  "exp": 1234567890,
  "iat": 1234567890,
  "scope": ["read", "write", "admin"]
}

# 权限装饰器
@require_permissions(["read"])
async def get_tasks(current_user: User = Depends(get_current_user)):
    pass
```

#### 3.2 核心API端点
```python
# 任务管理
POST   /api/v1/tasks                    # 创建任务
GET    /api/v1/tasks                    # 获取任务列表
GET    /api/v1/tasks/{task_id}          # 获取任务详情
PUT    /api/v1/tasks/{task_id}          # 更新任务
DELETE /api/v1/tasks/{task_id}          # 删除任务
POST   /api/v1/tasks/{task_id}/run      # 手动执行任务

# 指标分析
GET    /api/v1/metrics/dashboard        # 仪表盘数据
GET    /api/v1/metrics/sov              # SOV趋势数据
GET    /api/v1/metrics/accuracy         # 准确性数据
GET    /api/v1/metrics/comparison       # 模型对比数据

# 告警管理
GET    /api/v1/alerts                   # 获取告警列表
POST   /api/v1/alerts/rules             # 创建告警规则
PUT    /api/v1/alerts/rules/{rule_id}   # 更新告警规则
POST   /api/v1/alerts/test              # 测试告警配置

# 配置管理
GET    /api/v1/config/models            # 获取可用模型列表
POST   /api/v1/config/credentials       # 配置API密钥
GET    /api/v1/config/usage             # 获取使用统计
```

#### 3.3 数据模型设计
```python
# 监控任务模型
class MonitorTask(BaseModel):
    id: UUID
    tenant_id: UUID
    name: str
    description: Optional[str]
    schedule_cron: str
    is_active: bool
    created_at: datetime
    updated_at: datetime
    
    # 关联关系
    keywords: List[TaskKeyword]
    models: List[TaskModel]
    runs: List[TaskRun]

# 任务执行记录
class TaskRun(BaseModel):
    id: UUID
    task_id: UUID
    status: TaskStatus  # pending, running, completed, failed
    started_at: datetime
    completed_at: Optional[datetime]
    error_message: Optional[str]
    
    # 关联关系
    outputs: List[ModelOutput]
    metrics: List[MetricsSnapshot]

# 指标快照
class MetricsSnapshot(BaseModel):
    id: UUID
    run_id: UUID
    model_id: str
    keyword: str
    sov_score: float
    accuracy_score: int
    sentiment_score: float
    citation_rate: float
    positioning_hits: List[str]
    created_at: datetime
```

### 4. 数据库设计优化

#### 4.1 核心表结构
```sql
-- 监控任务表
CREATE TABLE monitor_tasks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id UUID NOT NULL,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    schedule_cron VARCHAR(100) NOT NULL DEFAULT '0 0 * * *',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- 任务-关键词关联表
CREATE TABLE task_keywords (
    task_id UUID REFERENCES monitor_tasks(id) ON DELETE CASCADE,
    keyword VARCHAR(255) NOT NULL,
    PRIMARY KEY (task_id, keyword)
);

-- 任务-模型关联表
CREATE TABLE task_models (
    task_id UUID REFERENCES monitor_tasks(id) ON DELETE CASCADE,
    model_id VARCHAR(100) NOT NULL,
    priority INTEGER DEFAULT 10,
    PRIMARY KEY (task_id, model_id)
);

-- 任务执行记录表
CREATE TABLE task_runs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    task_id UUID REFERENCES monitor_tasks(id) ON DELETE CASCADE,
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    started_at TIMESTAMPTZ DEFAULT NOW(),
    completed_at TIMESTAMPTZ,
    error_message TEXT,
    total_cost DECIMAL(10,4) DEFAULT 0.00
);

-- 模型输出表
CREATE TABLE model_outputs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    run_id UUID REFERENCES task_runs(id) ON DELETE CASCADE,
    model_id VARCHAR(100) NOT NULL,
    keyword VARCHAR(255) NOT NULL,
    prompt_text TEXT NOT NULL,
    raw_response TEXT NOT NULL,
    parsed_response JSONB,
    tokens_used INTEGER DEFAULT 0,
    cost DECIMAL(8,4) DEFAULT 0.00,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- 指标快照表
CREATE TABLE metrics_snapshot (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    run_id UUID REFERENCES task_runs(id) ON DELETE CASCADE,
    model_id VARCHAR(100) NOT NULL,
    keyword VARCHAR(255) NOT NULL,
    sov_score DECIMAL(5,2),
    accuracy_score INTEGER CHECK (accuracy_score >= 1 AND accuracy_score <= 10),
    sentiment_score DECIMAL(3,2) CHECK (sentiment_score >= -1 AND sentiment_score <= 1),
    citation_rate DECIMAL(5,2),
    positioning_hits JSONB DEFAULT '[]',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    UNIQUE (run_id, model_id, keyword)
);
```

#### 4.2 索引优化
```sql
-- 性能优化索引
CREATE INDEX idx_monitor_tasks_tenant_active ON monitor_tasks(tenant_id, is_active);
CREATE INDEX idx_task_runs_status_created ON task_runs(status, created_at);
CREATE INDEX idx_metrics_snapshot_composite ON metrics_snapshot(run_id, model_id, created_at);
CREATE INDEX idx_model_outputs_run_model ON model_outputs(run_id, model_id);

-- 时间序列查询优化
CREATE INDEX idx_metrics_snapshot_time_series ON metrics_snapshot(created_at, model_id, keyword);
```

#### 4.3 RLS安全策略
```sql
-- 启用行级安全
ALTER TABLE monitor_tasks ENABLE ROW LEVEL SECURITY;
ALTER TABLE task_runs ENABLE ROW LEVEL SECURITY;
ALTER TABLE metrics_snapshot ENABLE ROW LEVEL SECURITY;

-- 租户隔离策略
CREATE POLICY "tenant_isolation_tasks" ON monitor_tasks
    USING (tenant_id = current_setting('app.current_tenant_id')::UUID);

CREATE POLICY "tenant_isolation_runs" ON task_runs
    USING (EXISTS (
        SELECT 1 FROM monitor_tasks 
        WHERE monitor_tasks.id = task_runs.task_id 
        AND monitor_tasks.tenant_id = current_setting('app.current_tenant_id')::UUID
    ));
```

### 5. 部署和运维

#### 5.1 本地开发环境
```bash
# 启动开发环境
docker-compose up -d

# 初始化数据库
docker-compose exec backend python -m app.scripts.init_db

# 运行测试
docker-compose exec backend pytest
docker-compose exec frontend npm test
```

#### 5.2 生产环境部署

**Supabase配置:**
```sql
-- 启用必要的扩展
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_cron";

-- 配置定时任务
SELECT cron.schedule('geo-monitor-scheduler', '*/5 * * * *', 
    'SELECT net.http_post(
        url := ''https://your-backend.railway.app/api/v1/scheduler/trigger'',
        headers := ''{"Content-Type": "application/json", "Authorization": "Bearer " || current_setting(''app.scheduler_token'')}'',
        body := ''{}''
    );'
);
```

**环境变量配置:**
```bash
# 数据库
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key

# Redis
UPSTASH_REDIS_REST_URL=https://your-redis.upstash.io
UPSTASH_REDIS_REST_TOKEN=your-redis-token

# OpenRouter
OPENROUTER_API_KEY=sk-or-v1-your-key

# 应用配置
SECRET_KEY=your-super-secret-key-min-32-chars
ENVIRONMENT=production
DEBUG=false
```

#### 5.3 监控和告警
```yaml
# Grafana仪表盘配置
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    url: http://prometheus:9090

dashboards:
  - name: GEO Monitor
    panels:
      - title: API响应时间
        type: graph
        targets:
          - expr: histogram_quantile(0.95, http_request_duration_seconds_bucket)
      
      - title: 任务执行成功率
        type: stat
        targets:
          - expr: rate(task_success_total[5m]) / rate(task_total[5m])
      
      - title: OpenRouter API成本
        type: graph
        targets:
          - expr: increase(openrouter_cost_total[1h])
```

### 6. 测试策略

#### 6.1 单元测试
```python
# 后端测试示例
@pytest.mark.asyncio
async def test_create_task():
    task_data = {
        "name": "测试任务",
        "keywords": ["企业级CRM"],
        "models": ["openai/gpt-4"],
        "schedule": "0 0 * * *"
    }
    
    response = await client.post("/api/v1/tasks", json=task_data)
    assert response.status_code == 201
    assert response.json()["name"] == "测试任务"

@pytest.mark.asyncio
async def test_sov_calculation():
    # 测试SOV计算逻辑
    mentions = ["BrandA", "BrandB", "BrandA"]
    sov = calculate_sov("BrandA", mentions)
    assert sov == 66.67
```

```typescript
// 前端测试示例
describe('TaskForm', () => {
  it('should validate required fields', async () => {
    render(<TaskForm />);
    
    const submitButton = screen.getByRole('button', { name: /创建任务/i });
    fireEvent.click(submitButton);
    
    expect(screen.getByText('任务名称不能为空')).toBeInTheDocument();
    expect(screen.getByText('至少需要一个关键词')).toBeInTheDocument();
  });
  
  it('should submit form with valid data', async () => {
    const mockSubmit = jest.fn();
    render(<TaskForm onSubmit={mockSubmit} />);
    
    fireEvent.change(screen.getByLabelText('任务名称'), {
      target: { value: '测试任务' }
    });
    
    fireEvent.click(screen.getByRole('button', { name: /创建任务/i }));
    
    await waitFor(() => {
      expect(mockSubmit).toHaveBeenCalledWith({
        name: '测试任务',
        // ... other fields
      });
    });
  });
});
```

#### 6.2 集成测试
```python
# API集成测试
@pytest.mark.integration
async def test_task_execution_flow():
    # 1. 创建任务
    task = await create_test_task()
    
    # 2. 手动触发执行
    response = await client.post(f"/api/v1/tasks/{task.id}/run")
    assert response.status_code == 202
    
    # 3. 等待执行完成
    await wait_for_task_completion(task.id)
    
    # 4. 验证结果
    metrics = await client.get(f"/api/v1/metrics/task/{task.id}")
    assert metrics.status_code == 200
    assert len(metrics.json()["data"]) > 0
```

#### 6.3 端到端测试
```typescript
// E2E测试 (Playwright)
test('complete task creation and monitoring flow', async ({ page }) => {
  // 登录
  await page.goto('/login');
  await page.fill('[data-testid=email]', 'test@example.com');
  await page.fill('[data-testid=password]', 'password');
  await page.click('[data-testid=login-button]');
  
  // 创建任务
  await page.goto('/tasks/new');
  await page.fill('[data-testid=task-name]', '端到端测试任务');
  await page.fill('[data-testid=keywords]', '企业级CRM');
  await page.selectOption('[data-testid=models]', 'openai/gpt-4');
  await page.click('[data-testid=submit-button]');
  
  // 验证任务创建成功
  await expect(page.locator('[data-testid=success-message]')).toBeVisible();
  
  // 检查仪表盘更新
  await page.goto('/dashboard');
  await expect(page.locator('[data-testid=total-tasks]')).toContainText('1');
});
```

## 风险评估和缓解策略

### 1. 技术风险
- **API限流风险:** OpenRouter API可能存在限流，需要实现智能重试和降级机制
- **数据库性能:** 大量时序数据可能影响查询性能，需要合理的分区和索引策略
- **前端性能:** 大量图表渲染可能影响用户体验，需要虚拟化和懒加载

### 2. 业务风险
- **成本控制:** AI模型调用成本可能超预期，需要严格的成本监控和熔断机制
- **数据准确性:** 模型输出解析可能出错，需要多重验证和人工审核机制
- **用户体验:** 复杂的配置可能影响用户使用，需要简化流程和提供向导

### 3. 缓解措施
- **监控告警:** 全面的系统监控和实时告警
- **降级策略:** 关键功能的降级和备用方案
- **测试覆盖:** 高覆盖率的自动化测试
- **文档完善:** 详细的用户文档和开发文档

## 成功指标

### 1. 技术指标
- API响应时间 < 500ms (95th percentile)
- 系统可用性 > 99.5%
- 错误率 < 0.1%
- 测试覆盖率 > 80%

### 2. 业务指标
- 用户任务创建成功率 > 95%
- 监控数据准确性 > 90%
- 用户满意度 > 4.5/5
- 月活跃用户增长 > 20%

### 3. 性能指标
- 页面加载时间 < 2秒
- 数据查询响应时间 < 1秒
- 并发用户支持 > 100
- 数据处理吞吐量 > 1000 tasks/hour

## 总结

本开发计划基于现有的技术架构和白皮书要求，制定了详细的三阶段开发路线图。通过合理的技术选型、完善的功能设计和严格的质量控制，将构建一个功能完整、性能优异、用户体验良好的GEO监控系统。

项目预计总开发周期为9-13周，分为核心功能完善、高级功能开发和性能优化部署三个阶段。每个阶段都有明确的目标、任务清单和验收标准，确保项目按计划高质量交付。
